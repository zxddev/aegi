<!-- Author: msq -->

# 大模型选型理由与成本优化策略

> 更新日期：2026-02-06
> 信息来源：各厂商官方公告、TechCrunch、VentureBeat、NVIDIA NIM 等公开资料

## 1. 为什么需要顶级闭源大模型

aegi 是一个多代理编排平台（LangChain/LangGraph + MCP Gateway），核心能力依赖大模型的推理质量。以下场景对模型能力有硬性要求，开源模型目前无法可靠替代：

| 能力维度 | 具体要求 | 为什么不能用弱模型 |
|---------|---------|------------------|
| 复杂多步推理 | 代理规划、任务分解、跨模块决策 | 推理链断裂导致整个工作流失败，修复成本远高于模型调用成本 |
| 长上下文理解 | 跨文件代码分析、大文档摘要、多轮对话状态维持 | 上下文窗口不足或注意力衰减丢失关键信息 |
| 工具调用可靠性 | MCP 工具编排、函数调用参数生成、结构化输出 | 工具调用格式错误导致代理执行中断 |
| 代码生成质量 | 生成可直接运行的代码、精确的符号级编辑 | 低质量代码引入 bug 的修复成本远超调用费用 |

## 2. 各顶级模型定位与使用场景

### 2.1 GPT-5.3-Codex（OpenAI，2026-02-05 发布）

**核心定位：** 最强代理式编码模型，首个"参与构建自身"的模型。

**关键参数：**
- 上下文窗口：400K tokens
- SWE-Bench Pro / Terminal-Bench 行业最高分（跨 4 种语言）
- 首个被归类为"高网络安全能力"的模型，可识别软件漏洞
- 比 GPT-5.2-Codex 快 25%
- API 定价：尚未公布 5.3 独立定价；参考 GPT-5.2 为 $0.875/$7.00 per M tokens（input/output）

**在 aegi 中的使用场景：**
- 代理核心决策层（任务规划、方案选择、多约束权衡）
- 复杂代码生成与重构（跨模块改动、架构级变更）
- 安全审计与漏洞扫描
- 长时间运行的代理任务（研究、工具使用、复杂执行）

**选用理由：** SWE-Bench Pro 行业最高分，代理式编码能力最强，支持交互式协作（工作中可引导和交互而不丢失上下文）。

### 2.2 Claude Opus 4.6（Anthropic，2026-02-05 发布）

**核心定位：** 混合推理模型，长上下文 + 代理团队协作。

**关键参数：**
- 上下文窗口：1M tokens（beta），首个 Opus 级别百万 token 窗口
- 最大输出：128K tokens
- Terminal-Bench 2.0 最高分，Humanity's Last Exam 最高分
- MRCR v2 长上下文基准：76%（前代 Opus 4.5 仅 18.5%）
- 新增"Agent Teams"：多代理并行拆分任务
- API 定价：$5/$25 per M tokens（标准）；>200K tokens 时 $10/$37.50
- 可用平台：Claude API、AWS Bedrock、Google Vertex AI、Microsoft Foundry

**在 aegi 中的使用场景：**
- 长文档/大代码库分析（单次输入超 200K token 的场景）
- 代码审查与质量评估（`requesting-code-review` 流程）
- 多代理协作编排（利用 Agent Teams 特性）
- 深度推理与知识工作（金融分析、文档创建、多步研究工作流）
- 安全敏感操作的二次确认（高指令遵循度 + 低误对齐行为率）

**选用理由：** 1M token 上下文窗口质的飞跃（76% vs 18.5%），Agent Teams 原生支持多代理并行，与 aegi 的多代理编排架构天然契合。

### 2.3 Gemini 3 Pro（Google，2025-11-18 发布）

**核心定位：** 最强多模态推理 + Deep Think 深度思考。

**关键参数：**
- 上下文窗口：1M tokens
- 多模态：文本、图像、视频、音频、代码、PDF
- Deep Think 模式：多轮迭代推理，同时探索多个假设
- API 定价：约 $4/M tokens（大上下文场景）
- 可用平台：Google AI Studio、Vertex AI

**在 aegi 中的使用场景：**
- 多模态输入处理（截图分析、UI 走查、图表理解、视频内容分析）
- 超长上下文场景（整仓库级别的代码理解）
- Deep Think 深度推理（数学、科学、逻辑问题）
- 需要 Google 生态集成的场景（Vertex AI 部署）

**选用理由：** 多模态能力行业领先，Deep Think 模式适合需要深度推理的复杂场景，1M token 上下文窗口。

## 3. 开源模型降本场景（GLM-4.7 等）

### 3.1 GLM-4.7 概况（Z.ai / 智谱 AI，2025-12-22 发布）

**关键参数：**
- 总参数：358B（MoE 架构，32B 激活参数）
- 上下文窗口：200K tokens，最大输出 128K tokens
- Code Arena 开源模型排名第一
- SWE-bench Verified：73.8%
- 特色：Interleaved Thinking / Preserved Thinking / Turn-level Thinking
- 许可证：MIT（完全开源，可商用）
- API 成本：比 GPT-5.2 便宜 7-10 倍，比 Claude 便宜约 42 倍（Flash 版本）
- 轻量版 GLM-4.7-Flash：30B 总参数 / 3B 激活参数，智谱平台免费调用

**核心策略：将确定性高、复杂度低的任务下沉到 GLM-4.7，顶级模型只处理高价值决策。**

### 3.2 可直接替换的场景（成本节省 70-90%）

| 场景 | 说明 | 推荐模型 |
|------|------|---------|
| 文本分类/意图路由 | 判断用户请求类型，分发到对应代理 | GLM-4.7-Flash（免费）/ Qwen3 |
| 简单摘要/提取 | 从结构化文档中提取字段、生成摘要 | GLM-4.7-Flash |
| 模板化代码生成 | CRUD、配置文件、样板代码 | GLM-4.7 / DeepSeek V3.2 |
| Embedding 生成 | 文档/代码向量化用于检索 | BGE / GTE 等专用 embedding 模型 |
| 格式转换 | Markdown ↔ JSON、日志解析、数据清洗 | GLM-4.7-Flash |
| 翻译/润色 | 中英文档互译、文案润色（GLM-4.7 中文能力突出） | GLM-4.7 |

### 3.3 可部分替换的场景（成本节省 40-60%）

| 场景 | 策略 | 说明 |
|------|------|------|
| 多轮对话 | GLM-4.7 处理简单轮次，复杂轮次升级到闭源 | 动态路由，按复杂度切换 |
| 测试生成 | GLM-4.7 生成测试用例框架 | 单元测试模板化程度高，适合开源模型 |
| 文档生成 | GLM-4.7 初稿 → 人工/闭源模型校对 | docstring、README 等 |

### 3.4 不建议替换的场景（保持闭源）

- 代理核心决策链（错误代价高，省不了多少）
- 超长上下文分析（>200K tokens 场景需要 Opus 4.6 的 1M 窗口）
- 多代理团队编排（Opus 4.6 的 Agent Teams 原生支持）
- 多模态理解（图像/视频/音频分析需要 Gemini 3 Pro）






